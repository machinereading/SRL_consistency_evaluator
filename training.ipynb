{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from SRL_consistency_evaluator import dataio, converter\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import glob\n",
    "import os\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from transformers import *\n",
    "# from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "# from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "from tqdm import tqdm, trange\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from seqeval.metrics import f1_score\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train', required=True, help='검증팀 데이터 폴더')\n",
    "parser.add_argument('--model', required=False, help='모델이 저장되는 폴더', default='./model/')\n",
    "parser.add_argument('--epoch', required=False, default=3)\n",
    "parser.add_argument('--split', required=False, default=100)\n",
    "parser.add_argument('--batch', required=False, default=3)\n",
    "parser.add_argument('--n_split', required=False, default=False)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "batch_size = args.batch\n",
    "\n",
    "try:\n",
    "    dir_path = os.path.dirname(os.path.abspath( __file__ ))\n",
    "except:\n",
    "    dir_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fdir, split=100, n_split=False):\n",
    "    if n_split == False:\n",
    "        ori_data = converter.load_data(fdir, split=int(split))\n",
    "        tgt_data = dataio.data2tgt_data(ori_data)\n",
    "    else:\n",
    "        ori_data = converter.load_data(fdir, n_split=n_split)\n",
    "        tgt_data = []\n",
    "        for i in ori_data:\n",
    "            d = dataio.data2tgt_data(i)\n",
    "            tgt_data.append(d)\n",
    "    \n",
    "    return tgt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class for_BERT():\n",
    "    \n",
    "    def __init__(self, mode='training'):\n",
    "        self.mode = mode\n",
    "        \n",
    "        with open(dir_path+'/data/tag2idx.json','r') as f:\n",
    "            self.tag2idx = json.load(f)\n",
    "            \n",
    "        self.idx2tag = dict(zip(self.tag2idx.values(),self.tag2idx.keys()))\n",
    "        vocab_file_path = dir_path+'/data/bert-multilingual-cased-dict-add-frames'\n",
    "        self.tokenizer = BertTokenizer(vocab_file_path, do_lower_case=False, max_len=MAX_LEN)\n",
    "        self.tokenizer.additional_special_tokens = ['<tgt>', '</tgt>']\n",
    "        \n",
    "        \n",
    "    def idx2tag(self, predictions):\n",
    "        pred_tags = [self.idx2tag[p_i] for p in predictions for p_i in p]\n",
    "        \n",
    "        # bert tokenizer and assign to the first token\n",
    "    def bert_tokenizer(self, text):\n",
    "        orig_tokens = text.split(' ')\n",
    "        bert_tokens = []\n",
    "        orig_to_tok_map = []\n",
    "        bert_tokens.append(\"[CLS]\")\n",
    "        for orig_token in orig_tokens:\n",
    "            orig_to_tok_map.append(len(bert_tokens))\n",
    "            bert_tokens.extend(self.tokenizer.tokenize(orig_token))\n",
    "        bert_tokens.append(\"[SEP]\")\n",
    "\n",
    "        return orig_tokens, bert_tokens, orig_to_tok_map\n",
    "    \n",
    "    def convert_to_bert_input(self, input_data):\n",
    "        tokenized_texts, args = [],[]\n",
    "        orig_tok_to_maps = []\n",
    "        for i in range(len(input_data)):    \n",
    "            data = input_data[i]\n",
    "            text = ' '.join(data[0])\n",
    "            orig_tokens, bert_tokens, orig_to_tok_map = self.bert_tokenizer(text)\n",
    "            orig_tok_to_maps.append(orig_to_tok_map)\n",
    "            tokenized_texts.append(bert_tokens)\n",
    "\n",
    "            if self.mode == 'training':\n",
    "                ori_args = data[2]\n",
    "                arg_sequence = []\n",
    "                for i in range(len(bert_tokens)):\n",
    "                    if i in orig_to_tok_map:\n",
    "                        idx = orig_to_tok_map.index(i)\n",
    "                        ar = ori_args[idx]\n",
    "                        arg_sequence.append(ar)\n",
    "                    else:\n",
    "                        arg_sequence.append('X')\n",
    "                args.append(arg_sequence)\n",
    "\n",
    "        input_ids = pad_sequences([self.tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                              maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "        orig_tok_to_maps = pad_sequences(orig_tok_to_maps, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\", value=-1)\n",
    "        \n",
    "        if self.mode =='training':\n",
    "            arg_ids = pad_sequences([[self.tag2idx.get(ar) for ar in arg] for arg in args],\n",
    "                                    maxlen=MAX_LEN, value=self.tag2idx[\"X\"], padding=\"post\",\n",
    "                                    dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "        attention_masks = [[float(i>0) for i in ii] for ii in input_ids]    \n",
    "        data_inputs = torch.tensor(input_ids)\n",
    "        data_orig_tok_to_maps = torch.tensor(orig_tok_to_maps)\n",
    "        data_masks = torch.tensor(attention_masks)\n",
    "        \n",
    "        if self.mode == 'training':\n",
    "            data_args = torch.tensor(arg_ids)\n",
    "            bert_inputs = TensorDataset(data_inputs, data_orig_tok_to_maps, data_args, data_masks)\n",
    "        else:\n",
    "            bert_inputs = TensorDataset(data_inputs, data_orig_tok_to_maps, data_masks)\n",
    "        return bert_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_path='./model/', epochs=3, trn=False):\n",
    "    print('your model would be saved at', model_path)\n",
    "    \n",
    "    bert_io = for_BERT(mode='training')\n",
    "    \n",
    "    model = BertForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(bert_io.tag2idx))\n",
    "    model.to(device);\n",
    "    \n",
    "    trn_data = bert_io.convert_to_bert_input(trn)\n",
    "    sampler = RandomSampler(trn_data)\n",
    "    trn_dataloader = DataLoader(trn_data, sampler=sampler, batch_size=batch_size)\n",
    "    \n",
    "    # load optimizer\n",
    "    FULL_FINETUNING = True\n",
    "    if FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters()) \n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "    optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "    \n",
    "    \n",
    "    # train \n",
    "    max_grad_norm = 1.0\n",
    "    num_of_epoch = 0\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        # TRAIN loop\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(trn_dataloader):\n",
    "            # add batch to gpu\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_orig_tok_to_maps, b_input_args, b_input_masks = batch            \n",
    "            # forward pass\n",
    "            output = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_masks, labels=b_input_args)\n",
    "            loss = output[0]\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # track train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "#             break\n",
    "#         break\n",
    "\n",
    "        # print train loss per epoch\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "        num_of_epoch += 1\n",
    "    model.save_pretrained(model_path)\n",
    "    print('...training is done')\n",
    "    print('your model is saved to', model_path)\n",
    "    print('###################################################\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    fdir = args.train\n",
    "    model_path = args.model\n",
    "    epochs = args.epoch    \n",
    "    n_split = args.n_split\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    if model_path[-1] != '/':\n",
    "        model_path = model_path+'/'\n",
    "    \n",
    "    if n_split == False:\n",
    "        split = args.split\n",
    "        trn = load_data(fdir, split=split)\n",
    "        train(model_path, epochs=int(epochs), trn=trn)\n",
    "    else:\n",
    "        trns = load_data(fdir, n_split=int(n_split))\n",
    "        for i in range(len(trns)):\n",
    "            trn = trns[i]\n",
    "            model_path_split = model_path+'split_'+str(i)+'/'\n",
    "            if not os.path.exists(model_path_split):\n",
    "                os.makedirs(model_path_split)\n",
    "            train(model_path_split, epochs=int(epochs), trn=trn)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: 26394 annotations\n",
      "percent = 100.0%\n",
      "your model would be saved at ./model/\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ed4bd21d196e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-c498492d3844>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-f50d496d4a6b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_path, epochs, trn)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrn_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_bert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrn_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cf2ac9dd7925>\u001b[0m in \u001b[0;36mconvert_to_bert_input\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     67\u001b[0m             arg_ids = pad_sequences([[self.tag2idx.get(ar) for ar in arg] for arg in args],\n\u001b[1;32m     68\u001b[0m                                     \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                     dtype=\"long\", truncating=\"post\")\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mattention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
